{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import roc_auc_score\n\nimport xgboost as xgb\n\nfrom multiprocessing import Pool, cpu_count\n\n\nfrom keras.layers import Dense, Input, BatchNormalization, Embedding, Concatenate, Flatten, SpatialDropout1D\nfrom keras.models import Model\nfrom keras.optimizers import Adam\n\n# from tqdm import tqdm\n\n# !python -c \"import keras; print(keras.__version__)\"\n!python -c \"import torch; print(torch.__version__) \"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read/Merge Datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef get_datasets():\n    train_transaction = pd.read_csv('../input/train_transaction.csv', index_col='TransactionID')\n    test_transaction = pd.read_csv('../input/test_transaction.csv', index_col='TransactionID')\n\n    train_identity = pd.read_csv('../input/train_identity.csv', index_col='TransactionID')\n    test_identity = pd.read_csv('../input/test_identity.csv', index_col='TransactionID')\n\n    sample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')\n\n    train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n    test = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\n    print(train.shape)\n    print(test.shape)\n\n    y_train = train['isFraud'].copy()\n    del train_transaction, train_identity, test_transaction, test_identity\n\n    # Drop target, fill in NaNs\n    X_train = train.drop('isFraud', axis=1)\n    X_test = test.copy()\n\n    del train, test\n    \n    return X_train, y_train, X_test\n\n\nX_train, y_train, X_test = get_datasets()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Move Categorical Columns to the Left"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical = [\"ProductCD\", \"card1\", \"card2\", \"card3\", \"card4\", \"card5\", \"card6\", \"addr1\", \"addr2\",\n               \"P_emaildomain\", \"R_emaildomain\",\n              \"DeviceInfo\", \"DeviceType\"] + [\"id_0\" + str(i) for i in range(1, 10)] +\\\n                [\"id_\" + str(i) for i in range(10, 39)] + \\\n                 [\"M\" + str(i) for i in range(1, 10)]\n\n\nprint(categorical)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_numerical_columns(categorical_columns, df):\n    return [col for col in df.columns.values if col not in categorical_columns]\n\n\nnumerical = get_numerical_columns(categorical, X_train)\n\nprint(numerical)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndef move_columns_to_left(left_columns, df):\n    right_columns = [col for col in df.columns.values if col not in left_columns]\n    \n    return df[left_columns + right_columns]\n\n    \nX_train = move_columns_to_left(categorical, X_train)\nX_test = move_columns_to_left(categorical, X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fill NaN"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndef fill_nan_categorical(categorical_columns, df): \n    for f in categorical_columns:\n        df[f] = df[f].fillna(-999)\n        \n        \ndef fill_nan_numerical(numerical_columns, df):\n    for f in numerical_columns:\n        mean = int(df[f].mean())\n        df[f] = df[f].fillna(mean)\n        \n        \ndef fill_nan(categorical_columns, numerical_columns, df):\n    fill_nan_categorical(categorical_columns, df)\n    fill_nan_numerical(numerical_columns, df)\n    \n    \nfill_nan(categorical, numerical, X_train)\nfill_nan(categorical, numerical, X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%time\n# train_transaction = pd.read_csv('../input/train_transaction.csv', index_col='TransactionID')\n# test_transaction = pd.read_csv('../input/test_transaction.csv', index_col='TransactionID')\n\n# train_identity = pd.read_csv('../input/train_identity.csv', index_col='TransactionID')\n# test_identity = pd.read_csv('../input/test_identity.csv', index_col='TransactionID')\n\n# sample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')\n\n# train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n# test = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\n# print(train.shape)\n# print(test.shape)\n\n# y_train = train['isFraud'].copy()\n# del train_transaction, train_identity, test_transaction, test_identity\n\n# # Drop target, fill in NaNs\n# X_train = train.drop('isFraud', axis=1)\n# X_test = test.copy()\n\n# del train, test\n\n# X_train = X_train.fillna(-999)\n# X_test = X_test.fillna(-999)\n\n# Label Encoding\ncategory_size_dict = {}\n\nfor f in X_train.columns:\n    if X_train[f].dtype=='object' or X_test[f].dtype=='object' or f in categorical:\n        \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n        X_train[f] = lbl.transform(list(X_train[f].values))\n        X_test[f] = lbl.transform(list(X_test[f].values))  \n        \n        category_size_dict[f] = len(lbl.classes_)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clf = xgb.XGBClassifier(\n#     n_estimators=500,\n#     max_depth=9,\n#     learning_rate=0.05,\n#     subsample=0.9,\n#     colsample_bytree=0.9,\n#     missing=-999,\n#     random_state=2019,\n#     tree_method='gpu_hist'  # THE MAGICAL PARAMETER\n# )\n\n# %time clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del X_test\n\n# import gc\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def prediction_score(X, y, model):\n#     pred_prob = model.predict_proba(X)\n#     score = roc_auc_score(y, pred_prob[:, 1])\n    \n#     return score\n\n# pred_prob = clf.predict_proba(X_train)\n\n# score = roc_auc_score(y_train, pred_prob[:, 1])\n\n# print(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get Sizes of each categorical feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_category_sizes(categorical, category_sizes):\n    sizes = []\n    \n    for f in categorical:\n        sizes.append(category_sizes[f])\n        \n    return sizes\n    \n    \ncategory_sizes = get_category_sizes(categorical, category_size_dict)\n\nprint(category_sizes)\nprint(len(category_sizes))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_embedding_layers(vocab_sizes, input_length=1):\n    \"\"\"\n    Args:\n        embed_sizes (list): list of the number of each class\n    Return:\n        inputs (keras.layers.Input): shape is len(embed_sizes)\n        outputs (Tensor): shape should be np.sum(embed_sizes)\n    \"\"\"\n    \n    in_embeddings = []\n    out_embeddings = []\n    for i_input, input_dim in enumerate(vocab_sizes):\n        \n        in_embed = Input(shape=[1])\n        \n        # Do not shrink the size of input\n        if input_dim > 10000:\n            embed_size = (input_dim+1) // 50\n        elif input_dim > 1000:\n            embed_size = (input_dim+1) // 10\n        elif input_dim > 10:\n            embed_size = (input_dim+1) // 4\n        else:\n            embed_size = input_dim\n            \n        out_embed = Embedding(input_dim=input_dim, \n                              output_dim=embed_size)(in_embed)\n        \n        out_embed = (out_embed)\n        \n        print(in_embed.shape)\n        print(out_embed.shape)\n        \n        in_embeddings.append(in_embed)\n        out_embeddings.append(out_embed)\n        \n        \n    inputs = Concatenate(axis=-1)(in_embeddings)\n    outputs = Concatenate(axis=-1)(out_embeddings)\n    \n    return Flatten()(outputs), inputs\n\n\noutputs_embed, inputs_embed = build_embedding_layers(category_sizes)\n\nprint(inputs_embed.shape)\nprint(outputs_embed.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras.backend as k\n\ndef build_mixed_data_model(category_sizes, numeric_size, lr=1e-4):\n    \n    k.clear_session()\n    \n    # Build input layers\n    inputs_numeric = Input(shape=[numeric_size])\n    outputs_embed, inputs_category = build_embedding_layers(category_sizes)\n    \n    \n    inputs = Concatenate()([inputs_category, inputs_numeric])\n    \n#     x = Concatenate()([outputs_embed, inputs_numeric])\n    x = inputs\n    \n#     x = Dense(256, activation='relu')(x)\n#     x = BatchNormalization()(x)\n    \n#     x = Dense(128, activation='relu')(x)\n# #     x = BatchNormalization()(x)\n    \n#     x = Dense(64, activation='relu')(x)\n# #     x = BatchNormalization()(x)\n    \n#     x = Dense(16, activation='relu')(x)\n# #     x = BatchNormalization()(x)\n    \n    outputs = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inputs=inputs,\n                 outputs=outputs)\n    \n    optimizer = Adam(lr)\n    \n    model.compile(loss='binary_crossentropy',\n                 optimizer=optimizer,\n                 metrics=['accuracy'])\n    \n    return model\n\n\nsize_numerics = X_train.shape[1] - len(category_sizes)\nmodel = build_mixed_data_model(category_sizes, size_numerics)\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prediction_score(X, y, model, batch_size=128):\n    pred_prob = model.predict(X, batch_size)\n    score = roc_auc_score(y, pred_prob)\n    \n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nepochs=20\nbatch_size = 512\nprint('memory for batch_size {}: {:,}'.format(batch_size,\n                                              int(X_train.memory_usage().sum() / X_train.shape[0] * batch_size)))\n\n\nfor i_epoch in tqdm(range(epochs)):\n    model.fit(X_train, y_train,\n             epochs=1,\n             batch_size=batch_size)\n    \n    \n    print('roc-auc score: {}'.format(prediction_score(X_train, y_train, model)))\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluate ROC-AUC Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(prediction_score(X_train, y_train, model))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}